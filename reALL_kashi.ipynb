{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "再起型ニューラルネットワーク"
      ],
      "metadata": {
        "id": "ZKIhReB7HlB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 分かち書きを行うためにjanomeというPythonパッケージをインストール\n",
        "!pip install janome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxi16LiSHmAq",
        "outputId": "df19e99a-6b9a-47d2-90aa-2f2377642a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # CSVを含む表形式データを読み込み，分析するためのパッケージ\n",
        "from janome.tokenizer import Tokenizer # janomeからTokenizerクラスをimportする\n",
        "\n",
        "train_size = 1000 # 先頭の20,000件を訓練データとして用いることにする\n",
        "batch_size = 10 # ミニバッチのサイズを設定\n",
        "\n",
        "# CSVファイルを読み込み\n",
        "dataset = pd.read_csv('drive/MyDrive/実習B/full_AI.csv')\n",
        "\n",
        "# janomeはPythonで書かれた形態素解析器であり単語の分かち書きなどが可能\n",
        "# なお，速度に難があるため，大量のテキストを処理するときには，MeCabを利用すること\n",
        "tokenizer = Tokenizer(wakati=True)\n",
        "# text列の文字列を単語に分割し保存する\n",
        "dataset['text'] = dataset['text'].map(lambda x: list(tokenizer.tokenize(x)))\n",
        "\n",
        "# 読み込みんだCSVのうち，先頭の `train_size` 件を訓練データ，残りをテストデータとして用いる\n",
        "train_dataset, test_dataset = dataset[:train_size], dataset[train_size:]"
      ],
      "metadata": {
        "id": "ktVZ4fH7IDUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext.transforms as T\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.vocab import FastText\n",
        "from torchtext.vocab import vocab\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 単語埋め込みの読み込み\n",
        "fasttext = FastText(language=\"ja\")\n",
        "\n",
        "# fastText用の前処理を行う\n",
        "fasttext_vocab = vocab(fasttext.stoi)\n",
        "special_token_id = len(fasttext)\n",
        "fasttext_vocab.set_default_index(special_token_id)\n",
        "shape = fasttext.vectors.shape\n",
        "fasttext_vectors = torch.zeros((shape[0]+1, shape[1]))\n",
        "fasttext_vectors[:shape[0]] = fasttext.vectors\n",
        "\n",
        "# テキストの変換方法を定義\n",
        "text_transform = T.Sequential(\n",
        "    T.VocabTransform(fasttext_vocab), # 単語をfastTextの辞書で置き換え\n",
        "    T.ToTensor(padding_value=special_token_id) # (一番最後の整数値+1)という整数値で長さを揃える\n",
        ")\n",
        "\n",
        "# 各バッチからテキストとラベルを読み込む方法を定義\n",
        "def collate_batch(batch):\n",
        "    texts = text_transform([text for (id, label, text) in batch])\n",
        "    labels = torch.Tensor([label for (id, label, text) in batch])\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "# 訓練データ，および，テストデータ用のデータローダを用意する\n",
        "train_data_loader = DataLoader(train_dataset.values, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "test_data_loader = DataLoader(test_dataset.values, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUNGJ_zNINYY",
        "outputId": "059952a8-bbfd-4f29-d8c9-b373f21ca752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/wiki.ja.vec: 1.37GB [01:17, 17.8MB/s]                            \n",
            "  0%|          | 0/580000 [00:00<?, ?it/s]WARNING:torchtext.vocab.vectors:Skipping token b'580000' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|██████████| 580000/580000 [00:47<00:00, 12161.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データローダから読み込まれるデータ構造の確認\n",
        "for i, (texts, labels) in enumerate(train_data_loader):\n",
        "    print(texts.shape) # textsにどのようなデータが入っているか確認\n",
        "    for text, label in zip(texts, labels):\n",
        "        print(text, label)\n",
        "        break # 1テキストだけで終了\n",
        "    break # 1テキストだけで終了"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3HS-hblINff",
        "outputId": "c3bd92d4-c83c-4191-dd02-8d9b41a43a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 617])\n",
            "tensor([134348,   1291,  36282,   5973,     57,   3833, 367524,      3, 580000,\n",
            "           380,     10,  16315,      1,    440,    340,  47352,   1349,     11,\n",
            "          3960,     11,   1128,   9315,  10997, 580000,     20,   5056,  45927,\n",
            "             1,   5014,   1683,   6076,     16,     82,  14256,   1656,     96,\n",
            "          5360,    731,      1,    540,      6, 580000,     33,    494, 580000,\n",
            "            20,    110,     16,   3884,  12298,   5176,    365,      1,  27721,\n",
            "          1713,     20,    101,   2677,   1393,     12,  61849,   4506, 580000,\n",
            "            49,   5339,     10,   1561, 580000, 580000,   1286, 580000, 199748,\n",
            "            43,      1,     49,  22716,   1051,    453,   3760,     12, 580000,\n",
            "            16,     54, 580000,      1,     77,      3,  32698,      1,     77,\n",
            "             3,   4837,     72,     35, 580000,     13, 580000, 580000,     13,\n",
            "          4962,     10,     73,     20,   5730,     10,     27,     12,     35,\n",
            "        580000,     76,    353,   7599,      1,  26363,      9,  18515,   1235,\n",
            "           218,      6, 580000,     20,   1405,      3,    290,     20,     79,\n",
            "           327,      1,   1386,      1,    881,     70, 580000,   1946, 580000,\n",
            "            20,   5056, 580000,     15, 580000,     15, 580000,   1561,     35,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,   2189,\n",
            "        580000,     12,     46,     16,   5488,     12,   4614,   2936,    293,\n",
            "             6, 580000,     26,     29,     20,     28, 580000,     29,     16,\n",
            "          2511,      3,  20492,    293, 580000,     20,   3447,      9, 580000,\n",
            "            20, 580000,     16,    714,      6, 580000,    714,      6,   3993,\n",
            "         13155,     26,     29,     16,  10146,     35,  10136,      6,   4486,\n",
            "            20,    327,      1,   5091,     10,  42554,    119,      6, 580000,\n",
            "            20,     28,    118, 580000,    380,   7350,     15,  11768,   5578,\n",
            "            16,   8666,     10,   3040,      6, 580000,     12, 580000,     85,\n",
            "          9142,      1,   6560,      9, 580000, 580000,     29,     20,    824,\n",
            "           151,      6, 580000,     49,     12,   6686,      3,   9074, 259875,\n",
            "           155, 305293,     16,  55987,      1,   7698, 580000,     49,     12,\n",
            "        116310,   1051,     10, 580000,     20, 580000,     16,   1332,   8442,\n",
            "             1,    693,     10, 580000,     20,    129,      6, 580000,     20,\n",
            "        580000,     20,   2350, 580000,     29,     20,   5056,  13043,     33,\n",
            "          4925,   4467,   4216,     20,  25418,      6, 580000,   6762,     15,\n",
            "         20726,     12,   3025,   4045,   6122,      9,   6858,    220, 580000,\n",
            "           380,     62, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000]) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextClassifier, self).__init__()\n",
        "\n",
        "        # 単語埋め込み（FastText）を設定する\n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(\n",
        "            embeddings=fasttext_vectors, freeze=False)\n",
        "        \n",
        "        emb_size = fasttext_vectors.shape[1]\n",
        "        self.lstm = nn.LSTM(emb_size, 100) # LSTMを用意する\n",
        "        self.fc = nn.Linear(100, 1) # 1 x 100 の行列（この場合はベクトル）を含む全結合層を設定\n",
        "\n",
        "    def forward(self, seq):\n",
        "        x = self.embedding(seq) # 各単語を単語埋め込みに変換する\n",
        "        x = x.permute(1, 0, 2) # バッチサイズ x 系列長 x 埋め込みの次元数 -> 系列長 x バッチサイズ x 単語埋め込みの次元数\n",
        "        output, (h_n, c_n) = self.lstm(x) # LSTMを適用する\n",
        "        h = h_n.view(-1, 100) # 各系列の最後の入力（単語）に対応する隠れ状態は，1 x バッチサイズ x 隠れ状態の次元数，となっているため，これを，バッチサイズ x 隠れ状態の次元数，と変換する．\n",
        "        y = self.fc(F.relu(h)) # ReLUをかけてから，全結合層に通す．\n",
        "        y = y.squeeze() # yは バッチサイズ x 1 という行列になっているため，バッチサイズと同じ次元数を持つベクトルに変換\n",
        "        return y"
      ],
      "metadata": {
        "id": "qu8Jtz8FINmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # CPUもしくはGPUのどちらを使うかを設定\n",
        "model = TextClassifier() # ニューラルネットワークモデルのインスタンスを生成\n",
        "model = model.to(device) # CPUもしくはGPUのどちらを設定\n",
        "optimizer = optim.Adam(model.parameters()) # 基本的な学習方法はミニバッチ勾配降下法ではあるが，その中でもよく用いられるAdamと呼ばれる方法を用いることにする\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss() # 二値分類用の交差エントロピーを最小化することにする\n",
        "\n",
        "epoch_size = 20 # 勾配降下法はすべてのデータでパラメータを更新したら終わりではなく，全データでの更新（=1エポック）を複数回行う必要がある\n",
        "\n",
        "model.train() # モデルを学習モードに変更\n",
        "\n",
        "# `epoch_size`の数だけ以下を繰り返す\n",
        "for epoch in range(epoch_size):\n",
        "    losses = []\n",
        "    # イテレータはミニバッチ勾配降下法のために，`batch_size`で指定した数ごとにデータをわけて読み込んでくれる．\n",
        "    for batch_idx, (texts, labels) in enumerate(train_data_loader):\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() # 勾配の初期化\n",
        "        y = model(texts) # 現時点でのモデルの出力を得る\n",
        "        loss = criterion(y, labels.type(torch.float)) # 交差エントロピーの計算\n",
        "        loss.backward() # 交差エントロピーの勾配計算\n",
        "        optimizer.step() # パラメータ更新\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # 現在の交差エントロピーを出力\n",
        "    print('Epoch: {}\\tCross Entropy: {:.6f}'.format(epoch, sum(losses)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7vJ2E4zINtW",
        "outputId": "191d5ce0-3c22-4691-84be-6cae26c9f20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\tCross Entropy: 59.838763\n",
            "Epoch: 1\tCross Entropy: 57.271819\n",
            "Epoch: 2\tCross Entropy: 49.984273\n",
            "Epoch: 3\tCross Entropy: 39.285474\n",
            "Epoch: 4\tCross Entropy: 40.967405\n",
            "Epoch: 5\tCross Entropy: 36.431937\n",
            "Epoch: 6\tCross Entropy: 32.085296\n",
            "Epoch: 7\tCross Entropy: 26.441701\n",
            "Epoch: 8\tCross Entropy: 24.276287\n",
            "Epoch: 9\tCross Entropy: 34.252574\n",
            "Epoch: 10\tCross Entropy: 30.568636\n",
            "Epoch: 11\tCross Entropy: 26.569189\n",
            "Epoch: 12\tCross Entropy: 29.688327\n",
            "Epoch: 13\tCross Entropy: 27.453540\n",
            "Epoch: 14\tCross Entropy: 25.940174\n",
            "Epoch: 15\tCross Entropy: 29.285921\n",
            "Epoch: 16\tCross Entropy: 27.447111\n",
            "Epoch: 17\tCross Entropy: 28.892248\n",
            "Epoch: 18\tCross Entropy: 29.700594\n",
            "Epoch: 19\tCross Entropy: 28.007326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "model.eval() # モデルを評価モードに変更\n",
        "for batch_idx, (texts, labels) in enumerate(test_data_loader):\n",
        "    texts, labels = texts.to(device), labels.to(device)\n",
        "    y = model(texts) # モデルの出力を得る\n",
        "    result = torch.sigmoid(y) # `TextClassifier`ではsigmoid関数を適用していなかったのでここで適用\n",
        "    prediction = result >= 0.5 # `result`ベクトルと同じ次元を持ち，`result`の中で0.5以上である次元がTrue，それ以外がFalseであるベクトルを`prediction`とする\n",
        "    target = labels == 1 # `labels`ベクトルと同じ次元を持ち，`labels`の中で1である次元がTrue，それ以外がFalseであるベクトルを`target`とする\n",
        "    correct_num = target.eq(prediction).sum().item() # `prediction`ベクトルと`target`ベクトルでTrue/Falseが一致したものの数を数える\n",
        "    correct += correct_num\n",
        "\n",
        "# test_iterator.datasetにはテストデータ全体が入っているので，これの長さはテストデータの事例数となる\n",
        "print(\"Accuracy: {:.3f}\".format(correct / len(test_data_loader.dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3kXr6e-IN03",
        "outputId": "2637955d-ea02-4f4c-ccf1-5ba1babe8e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bST0M2eiSRGw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}