{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "再起型ニューラルネットワーク"
      ],
      "metadata": {
        "id": "ZKIhReB7HlB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 分かち書きを行うためにjanomeというPythonパッケージをインストール\n",
        "!pip install janome"
      ],
      "metadata": {
        "id": "kxi16LiSHmAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f5fcfc-fc00-4eb2-f777-bd1fafa2c943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.8/dist-packages (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # CSVを含む表形式データを読み込み，分析するためのパッケージ\n",
        "from janome.tokenizer import Tokenizer # janomeからTokenizerクラスをimportする\n",
        "\n",
        "train_size = 25000 # 先頭の20,000件を訓練データとして用いることにする\n",
        "batch_size = 32 # ミニバッチのサイズを設定\n",
        "\n",
        "# CSVファイルを読み込み\n",
        "dataset = pd.read_csv('drive/MyDrive/実習B/slice_AI.csv')\n",
        "\n",
        "# janomeはPythonで書かれた形態素解析器であり単語の分かち書きなどが可能\n",
        "# なお，速度に難があるため，大量のテキストを処理するときには，MeCabを利用すること\n",
        "tokenizer = Tokenizer(wakati=True)\n",
        "# text列の文字列を単語に分割し保存する\n",
        "dataset['text'] = dataset['text'].map(lambda x: list(tokenizer.tokenize(x)))\n",
        "\n",
        "# 読み込みんだCSVのうち，先頭の `train_size` 件を訓練データ，残りをテストデータとして用いる\n",
        "train_dataset, test_dataset = dataset[:train_size], dataset[train_size:]"
      ],
      "metadata": {
        "id": "ktVZ4fH7IDUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext.transforms as T\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.vocab import FastText\n",
        "from torchtext.vocab import vocab\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 単語埋め込みの読み込み\n",
        "fasttext = FastText(language=\"ja\")\n",
        "\n",
        "# fastText用の前処理を行う\n",
        "fasttext_vocab = vocab(fasttext.stoi)\n",
        "special_token_id = len(fasttext)\n",
        "fasttext_vocab.set_default_index(special_token_id)\n",
        "shape = fasttext.vectors.shape\n",
        "fasttext_vectors = torch.zeros((shape[0]+1, shape[1]))\n",
        "fasttext_vectors[:shape[0]] = fasttext.vectors\n",
        "\n",
        "# テキストの変換方法を定義\n",
        "text_transform = T.Sequential(\n",
        "    T.VocabTransform(fasttext_vocab), # 単語をfastTextの辞書で置き換え\n",
        "    T.ToTensor(padding_value=special_token_id) # (一番最後の整数値+1)という整数値で長さを揃える\n",
        ")\n",
        "\n",
        "# 各バッチからテキストとラベルを読み込む方法を定義\n",
        "def collate_batch(batch):\n",
        "    texts = text_transform([text for (id, label, text) in batch])\n",
        "    labels = torch.Tensor([label for (id, label, text) in batch])\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "# 訓練データ，および，テストデータ用のデータローダを用意する\n",
        "train_data_loader = DataLoader(train_dataset.values, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "test_data_loader = DataLoader(test_dataset.values, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "VUNGJ_zNINYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af4ff0c7-7026-46c7-c0fc-e40615a0c766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/wiki.ja.vec: 1.37GB [02:32, 9.00MB/s]                            \n",
            "  0%|          | 0/580000 [00:00<?, ?it/s]WARNING:torchtext.vocab.vectors:Skipping token b'580000' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|██████████| 580000/580000 [00:43<00:00, 13253.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データローダから読み込まれるデータ構造の確認\n",
        "for i, (texts, labels) in enumerate(train_data_loader):\n",
        "    print(texts.shape) # textsにどのようなデータが入っているか確認\n",
        "    for text, label in zip(texts, labels):\n",
        "        print(text, label)\n",
        "        break # 1テキストだけで終了\n",
        "    break # 1テキストだけで終了"
      ],
      "metadata": {
        "id": "G3HS-hblINff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e5bf5e-41bc-4af1-93a4-78f9e004cb78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 64])\n",
            "tensor([580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000, 580000,\n",
            "        580000]) tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextClassifier, self).__init__()\n",
        "\n",
        "        # 単語埋め込み（FastText）を設定する\n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(\n",
        "            embeddings=fasttext_vectors, freeze=False)\n",
        "        \n",
        "        emb_size = fasttext_vectors.shape[1]\n",
        "        self.lstm = nn.LSTM(emb_size, 100) # LSTMを用意する\n",
        "        self.fc = nn.Linear(100, 1) # 1 x 100 の行列（この場合はベクトル）を含む全結合層を設定\n",
        "\n",
        "    def forward(self, seq):\n",
        "        x = self.embedding(seq) # 各単語を単語埋め込みに変換する\n",
        "        x = x.permute(1, 0, 2) # バッチサイズ x 系列長 x 埋め込みの次元数 -> 系列長 x バッチサイズ x 単語埋め込みの次元数\n",
        "        output, (h_n, c_n) = self.lstm(x) # LSTMを適用する\n",
        "        h = h_n.view(-1, 100) # 各系列の最後の入力（単語）に対応する隠れ状態は，1 x バッチサイズ x 隠れ状態の次元数，となっているため，これを，バッチサイズ x 隠れ状態の次元数，と変換する．\n",
        "        y = self.fc(F.relu(h)) # ReLUをかけてから，全結合層に通す．\n",
        "        y = y.squeeze() # yは バッチサイズ x 1 という行列になっているため，バッチサイズと同じ次元数を持つベクトルに変換\n",
        "        return y"
      ],
      "metadata": {
        "id": "qu8Jtz8FINmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # CPUもしくはGPUのどちらを使うかを設定\n",
        "model = TextClassifier() # ニューラルネットワークモデルのインスタンスを生成\n",
        "model = model.to(device) # CPUもしくはGPUのどちらを設定\n",
        "optimizer = optim.Adam(model.parameters()) # 基本的な学習方法はミニバッチ勾配降下法ではあるが，その中でもよく用いられるAdamと呼ばれる方法を用いることにする\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss() # 二値分類用の交差エントロピーを最小化することにする\n",
        "\n",
        "epoch_size = 20 # 勾配降下法はすべてのデータでパラメータを更新したら終わりではなく，全データでの更新（=1エポック）を複数回行う必要がある\n",
        "\n",
        "model.train() # モデルを学習モードに変更\n",
        "\n",
        "# `epoch_size`の数だけ以下を繰り返す\n",
        "for epoch in range(epoch_size):\n",
        "    losses = []\n",
        "    # イテレータはミニバッチ勾配降下法のために，`batch_size`で指定した数ごとにデータをわけて読み込んでくれる．\n",
        "    for batch_idx, (texts, labels) in enumerate(train_data_loader):\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() # 勾配の初期化\n",
        "        y = model(texts) # 現時点でのモデルの出力を得る\n",
        "        loss = criterion(y, labels.type(torch.float)) # 交差エントロピーの計算\n",
        "        loss.backward() # 交差エントロピーの勾配計算\n",
        "        optimizer.step() # パラメータ更新\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # 現在の交差エントロピーを出力\n",
        "    print('Epoch: {}\\tCross Entropy: {:.6f}'.format(epoch, sum(losses)))\n"
      ],
      "metadata": {
        "id": "u7vJ2E4zINtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1923d10c-0930-4e82-e504-d243724447cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\tCross Entropy: 339.739955\n",
            "Epoch: 1\tCross Entropy: 292.401647\n",
            "Epoch: 2\tCross Entropy: 255.345709\n",
            "Epoch: 3\tCross Entropy: 206.589820\n",
            "Epoch: 4\tCross Entropy: 136.217601\n",
            "Epoch: 5\tCross Entropy: 89.109435\n",
            "Epoch: 6\tCross Entropy: 76.721182\n",
            "Epoch: 7\tCross Entropy: 68.483462\n",
            "Epoch: 8\tCross Entropy: 62.532087\n",
            "Epoch: 9\tCross Entropy: 60.494997\n",
            "Epoch: 10\tCross Entropy: 57.860027\n",
            "Epoch: 11\tCross Entropy: 55.185145\n",
            "Epoch: 12\tCross Entropy: 55.506827\n",
            "Epoch: 13\tCross Entropy: 55.487376\n",
            "Epoch: 14\tCross Entropy: 49.551541\n",
            "Epoch: 15\tCross Entropy: 49.980563\n",
            "Epoch: 16\tCross Entropy: 55.254276\n",
            "Epoch: 17\tCross Entropy: 47.211614\n",
            "Epoch: 18\tCross Entropy: 96.775977\n",
            "Epoch: 19\tCross Entropy: 52.284536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "model.eval() # モデルを評価モードに変更\n",
        "for batch_idx, (texts, labels) in enumerate(test_data_loader):\n",
        "    texts, labels = texts.to(device), labels.to(device)\n",
        "    y = model(texts) # モデルの出力を得る\n",
        "    result = torch.sigmoid(y) # `TextClassifier`ではsigmoid関数を適用していなかったのでここで適用\n",
        "    prediction = result >= 0.5 # `result`ベクトルと同じ次元を持ち，`result`の中で0.5以上である次元がTrue，それ以外がFalseであるベクトルを`prediction`とする\n",
        "    target = labels == 1 # `labels`ベクトルと同じ次元を持ち，`labels`の中で1である次元がTrue，それ以外がFalseであるベクトルを`target`とする\n",
        "    correct_num = target.eq(prediction).sum().item() # `prediction`ベクトルと`target`ベクトルでTrue/Falseが一致したものの数を数える\n",
        "    correct += correct_num\n",
        "\n",
        "# test_iterator.datasetにはテストデータ全体が入っているので，これの長さはテストデータの事例数となる\n",
        "print(\"Accuracy: {:.3f}\".format(correct / len(test_data_loader.dataset)))"
      ],
      "metadata": {
        "id": "b3kXr6e-IN03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3df133-2fb0-4309-f734-008fc4510423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oqJzAxrpOJvV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}